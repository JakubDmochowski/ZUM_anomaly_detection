---
title: "Przykładowy przebieg doboru hiperparametrów"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
require(tidyverse)
library(rhdf5)
library(factoextra)
library(NbClust)
library(e1071)
library(dbscan)
library(mclust)
library(DDoutlier)
library(dplyr)
library(plotly)
library(ROCR)
library(psych)
library(knitr)
library(data.table)
library(caTools)
library(partykit)
```

```{r load data}
mammography_x = read.csv(file = 'mammography_x.csv', header=FALSE)
mammography_y = read.csv(file = 'mammography_y.csv', header=FALSE)

mammography = mammography_x %>% mutate(class = mammography_y$V1)
```

Aby lepiej zrozumiec charakterystyke danych, wyswietlmy je i zaznaczmy znane anomalie zanim przejdziemy do nienadzorowanej detekcji anomalii.


```{r show data}
plot_ly(mammography,
        x = ~V1,
        y = ~V2,
        z = ~V3,
        color=~class,
        colors=c('#ffff00', '#000000'),
        opacity=0.7,
        size=1
      )
```

W przypadku danych o więcej niż 3 wymiarach, ciężko jest zwizualizować dane tak by widać było anomalie na pierwszy rzut oka.

Dowiedzmy się przynajmniej ile takich anomalii w zbiorze jest, aby poznać skalę problemu.

```{r}
sum(mammography_y)
```

Podział zbioru danych na treningowe i testowe z informacją o ilości anomalii w podzbiorach.

```{r}
split_ratio = 0.8

set.seed(123)
split = sample.split(mammography, SplitRatio = split_ratio)
data_train = subset(mammography, split == TRUE)
data_train_x = data_train %>% select(1:6)
data_train_y = data_train %>% select(7)

data_test = subset(mammography, split == FALSE)
data_test_x = data_test %>% select(1:6)
data_test_y = data_test %>% select(7)

sum(data_train_y)
sum(data_test_y)
```

## Grupowanie

##### K-Means
Do pogrupowania danych z użyciem algorytmu centroidów, trzeba wiedzieć ile takich centroidów ma być w danych.
Sprawdzimy to z wykorzystaniem funkcji fviz_nbclust z pakietu factoextra.
Jako, że można użyć kilku metod do sprawdzenia optymalnej ilości klastrów, sprawdźmy więcej niż jedną.

```{r}
fviz_nbclust(data_train_x, kmeans, method = "silhouette")
```

```{r}
fviz_nbclust(data_train_x, kmeans, method = "wss")
```

```{r number of clusters}
fviz_nbclust(data_train_x, kmeans, method = "gap_stat", k.max=7, nboot=30)
```

Zwizualizujmy, jak wygląda grupowanie dla wybranej ilości centroidów.

```{r}
no_clusters = 3
cluster_data = kmeans(data_train_x, no_clusters, algorithm="Hartigan-Wong")
fviz_cluster(cluster_data, data = data_train_x)
```

Tutaj, aby wybrać anomalie spośród punktów w grupach należałoby dobrać jakąś miarę niepodobieństwa.
Określmy przykładową miarę niepodobieństwa jako odległość danej od środka grupy do której została zakwalifikowana.

```{r}
cluster_centers = cluster_data$centers
data = data_train
data %<>% mutate(group = cluster_data$cluster) %>% mutate(dist = sqrt((V1-cluster_centers[group, 1])^2 + (V2-cluster_centers[group, 2])^2 + (V3-cluster_centers[group, 3])^2 + (V4-cluster_centers[group, 4])^2)) %>% arrange(dist)
data$dist = data$dist / max(data$dist)
plot(data$dist)
```

Miara niepodobieństwa mapuje nam dane wielowymiarowe na jeden wymiar. 

```{r}
plot_ly(data,
        x = ~V1,
        y = ~V2,
        z = ~V3,
        color=~dist,
        colors=c('#ffff00', '#000000'),
        opacity=0.7,
        size=1
      )
```

Jednowymiarowe dane jesteśmy w stanie porównać, dzięki czemu możemy określić jakiś próg, powyżej którego będziemy dane traktowali jako anomalie.
Po zakwalifikowaniu anomalii, możemy porównać wyniki z faktycznie wykrytymi anomaliami podanymi w danych wejściowych. W ten sposób dowiemy się, jaka jest jakość modelu a-priori.
Na tym etapie wybieramy jakimi miarami jakości chcielibyśmy się posłużyć. W naszym przypadku wybieramy analizę ROC oraz miarę F1.


```{r}
outlier_threshold = 0.35
data = data %>% mutate(pred_int = as.integer(dist > outlier_threshold))

pred = prediction(predictions=data$dist, labels=data$class)
perf = performance(pred, "tpr", "fpr")
plot(perf)
abline(0,1,col="#0000ff")
```

Sprawdźmy jeszcze miarę F1. Zwraca wysokie wyniki tylko, gdy jednoczenie oba współczynniki, precyzji i odzysku, dają wysokie wyniki.

```{r}
data = data %>% mutate(tp = class & pred_int)
data = data %>% mutate(fp = !class & pred_int)
data = data %>% mutate(tn = !class & !pred_int)
data = data %>% mutate(fn = class & !pred_int)
recall = tally(data, tp) / (tally(data, tp) + tally(data, fn))
precision = tally(data, tp) / (tally(data, tp) + tally(data, fp))
f_value = 2 * recall * precision / (recall + precision)
f_value

```

Na tym etapie mamy wyniki jakie metody grupowania mogłyby dać dla tego zbioru danych.
Są to niestety miary które niewiele mówią o jakości predykcji dla nowych danych.
Są one w zasadzie tylko poglądowe, aby upewnić się że można wytrenować model w sposób nienadzorowany metodami grupowania,
tak aby wyniki detekcji anomalii na podstawie tego modelu były nielosowe i relatywnie poprawne.

W tym miejscu warto zaznaczyć, że wynik NaN oznacza, że i precyzja i odzysk osiągnęły wartość 0, czyli nie było poprawnie zakwalifikowanej nawet jednej anomalii.
W pozostałych przypadkach, określona wartość będzie plasowała się w przedziale [0,1], gdzie 0 będzie oznaczało jakość niską, a 1 wysoką.

Natomiast, aby określić jakość modelu, musielibyśmy zastosować go dla nowych danych i sprawdzić czy predykcje okazałyby się identyczne z faktycznym wynikiem kwalifikacji anomalii.

Aby określić jak model radzi sobie z nowymi danymi, będziemy musieli go zastosować dla zbioru danych który nie został wzięty pod uwagę przy tworzeniu modelu.



```{r}
outlier_threshold = 0.4
predict.kmeans <- function(object, newdata){
    centers <- object$centers
    n_centers <- nrow(centers)
    dist_mat <- as.matrix(dist(rbind(centers, newdata)))
    dist_mat <- dist_mat[-seq(n_centers), seq(n_centers)]
    max.col(-dist_mat)
}

groups = predict(cluster_data, data_test_x)
data = data_test %>% mutate(group = groups)
data %<>% mutate(dist = sqrt((V1-cluster_centers[group, 1])^2 + (V2-cluster_centers[group, 2])^2 + (V3-cluster_centers[group, 3])^2 + (V4-cluster_centers[group, 4])^2)) %>% arrange(dist)
data$dist = data$dist / max(data$dist)
data = data %>% mutate(pred_int = as.integer(dist > outlier_threshold))
plot(data$dist)
```

Zrobiliśmy predykcje grup na podstawie modelu, zaaplikowaliśmy miarę niepodobieństwa na wyniki danych testowych. Rozkład posortowanych miar niepodobieństwa dla danych testowych prezentuje się jak wyżej.

Aby określić jakość predykcji, zastosujemy analizę roc oraz wyliczymy miarę F1.
```{r}
pred = prediction(predictions=data$dist, labels=data$class)
perf = performance(pred, "tpr", "fpr")
plot(perf)
abline(0,1,col="#0000ff")
```

```{r}

data = data %>% mutate(tp = class & pred_int)
data = data %>% mutate(fp = !class & pred_int)
data = data %>% mutate(tn = !class & !pred_int)
data = data %>% mutate(fn = class & !pred_int)
recall = tally(data, tp) / (tally(data, tp) + tally(data, fn))
precision = tally(data, tp) / (tally(data, tp) + tally(data, fp))
f_value = 2 * recall * precision / (recall + precision)
f_value
```


Miary jakości powyżej oznaczają, że detekcja anomalii z użyciem takich parametrów, miary niepodobieństwa w postaci odległości od najbliższego klastra i z zastosowaniem grupowania centroidami nie przynosi znaczącego uzysku ponad losowe wybieranie anomalii spośród danych.
Może być to spowodowane, że algorytm centroidów jest słabo dostosowany do charakterystyki danych tego zbioru, jak również słabym dopasowaniem miary niepodobieństwa.

Możliwe, że w przypadku tego zbioru danych zastosowanie innych algorytmów grupowania oraz innych miar niepodobieństwa do grup okazałoby się jakościowo lepszym rozwiązaniem.

## Klasyfikacja

Na tych samych zbiorach danych uruchomiono nadzorowane algorytmy klasyfikacji w celu porównania ich skuteczności z algorytmami grupującymi uczonymi bez nadzoru.
W tym celu wykorzystano wbudowane metody klasyfikacji:
* `svm` (Support Vector Machine) z funkcją jądrową wielomianową
* `naiveBayes` (Naiwny klasyfikator Bayesowski)
* `ctree` (Conditional Inference Tree)

 

Do każdej z metod podano wartości ze zbioru treningowego wraz z etykietą przynależności do klasy anomalii, a następnie dokonano predyckji klasy dla punktów ze zbioru testowego.
Zbiory były takie same jak podczas nienadzorowanego grupowania.

 

Dla każdej z metod dokonano pomiaru wskażnika F na podstawie macierzy błędów oraz wyznaczono kształt krzywej ROC.

Do określania miary F1 użyliśmy definicji funkcji:

```{r}
f_value <- function (cm) {
  fp = cm[1,2]
  fn = cm[2,1]
  tp = cm[2,2]
  recall = tp / (tp+fn)
  precision = tp / (tp+fp)
  f = (2 * recall * precision) / (recall + precision)
  return(f)
}
```

#### Algorytmy

##### Support Vector Machines (SVM)

Metoda ma za zadanie wyznaczyć hiperpłaszczyznę oddzielającą przykłady należące do dwóch klas w taki sposób aby uzyskać między nimi największy margines.
Funkcja jądrowa ma zadanie przekształcić dane wejściowe do takiej przestrzeni, aby możliwe było wyznaczenie płaszczyzny która oddziela najwięcej punktów.

```{r}
svm.model = svm(as.factor(class) ~ ., data = data_train, type = 'C-classification', kernel = 'polynomial', probability = TRUE)
svm.pred = predict(svm.model, newdata = data_test_x)
svm.cm = table(data_test_y$class, svm.pred)
f_value(svm.cm)
```

##### Naiwny klasyfikator Bayesowski

Metoda zakłada niezależność zmiennych decyzyjnych, i wyznacza na ich podstawie prawdopodobieństwo przynależności przynależności punktu wejściowego do danej klasy,
tworząc modele rozkładów ze na podstawie przykładów ze zbioru uczącego. Ponieważ jedną z dwu klas stanowią anomalie, modelowanie ich rozkładu
z definiji powinno okazać się nieskuteczne.

```{r}
nvb.model = naiveBayes(as.factor(class) ~ ., data = data_train)
nvb.pred = predict(nvb.model, newdata = data_test_x)
nvb.cm = table(data_test_y$class, nvb.pred)
f_value(nvb.cm)
```

##### Conditional Inference Tree (ctree)

Drzewo wnioskowania warunkowego jest drzewem decyzyjnym konstruującym kolejne poziomy podziału na podstawie statystycznego testu niezależności zmiennych decyzyjnych od klasy.
Dla każdego poziomu decyzji wybierana jest ta zmienna decyzyjna dla której test niezależności klasy od zmiennej ma najwyższą istotność (zmienna jest silnie związana z klasą).
Tak skonstuowane drzewo nie wymaga dlaszego przycinania.

```{r}
ct.model = ctree(as.factor(class) ~ ., data = data_train)
ct.pred = predict(ct.model, newdata = data_test_x)
ct.cm = table(data_test_y$class, ct.pred)
f_value(ct.cm)
```

#### Jakość modeli

Analiza ROC pozwala nam zobaczyć jakość predykcji danego algorytmu. Im większe jest pole pod wykresem, tym wyższa jakość modelu.

```{r}
require(ROCR)
svm.prob = predict(svm.model, type="prob", newdata= data_test_x, probability = TRUE)
svm.rocr = prediction(attr(svm.prob, "probabilities")[,2], data_test_y$class)
svm.perf = performance(svm.rocr, "tpr","fpr")
nvb.prob = predict(nvb.model, newdata=data_test_x, type="raw")
nvb.rocr = prediction(nvb.prob[,2], data_test_y$class)
nvb.perf = performance(nvb.rocr, "tpr","fpr")
ct.prob = predict(ct.model, newdata=data_test_x, type="prob")
ct.rocr = prediction(ct.prob[,2], data_test_y$class)
ct.perf = performance(ct.rocr, "tpr","fpr")
plot(svm.perf, col=4, main="ROC curves of different machine learning classifier")
legend(0.6, 0.6, c('svm', 'ctree', 'Naive Bayes'), 4:6)
plot(ct.perf, col=5, add=TRUE)
plot(nvb.perf, col=6, add=TRUE)
```

